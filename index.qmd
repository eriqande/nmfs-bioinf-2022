# Welcome! {.unnumbered}

Welcome to the website for the National Marine Fisheries Service
Linux, Slurm, and Bioinformatics training to be held virtually
over three days:

* Day 1: Monday, October 17, 2022. 10 AM - 12 PM PDT
* Day 2: Tuesday, October 18, 2022. 10 AM - 12:30 PM PDT
* Day 3: Wednesday, October 19, 2022. 10 AM - 12 PM PDT


## Prerequisites

This course will be using the SEDNA high-performance computing
cluster located at the Northwest Fisheries Science Center. This cluster
(and, hence, this course)
is only available to NMFS employees and affiliates. If you are a NMFS
employee and you are interested in this course, please see 
[here](https://docs.google.com/document/d/1nn0T0OWEsQCBoCdaH6DSY69lQSbK3XnPlseyyQuU2Lc/edit#heading=h.qpx57rvxr0sj) for information about how to
get an account on the cluster. 

This course is intended for people who have already had some exposure
to Unix or Linux.  You should be reasonably comfortable navigating around 
the Unix filesystem using the command line.
For a refresher, please read [this chapter from my
online bioinformatics book](https://eriqande.github.io/eca-bioinf-handbook/essential-unixlinux-terminal-knowledge.html).

My goal is to:

- teach the shell programming constructs and
the text processing tricks that I find myself using all the time in my 
day-to-day work
- provide an introduction to how to use SLURM to do cluster computing
- On the last day, show how Snakemake works, and how it can be used on the
SEDNA cluster to simplify your bioinformatics life.

## Proposed Course Topics


* Day 1: Intro, Unix-review, shell programming, `awk`
    - Introduction to the SEDNA cluster (15 minutes Krista and Giles)
        - Cluster infrastructure and configuration.
        - Scientific software and the installation requests
    - [Quick Unix Review](quick-unix-review.qmd#quick-unix-review) (25 minutes)
    - [Shell Programming](shell-prog.qmd#shell-prog) (50 Minutes)
    - [Processing text files with `awk`](awk-intro.qmd#awk-intro) (30 minutes)
* Day 2: A little bash stuff, then SLURM, SLURM, SLURM
    - [Shell scripts and bash functions](scripts-and-functions.qmd#script-func)
    - Computing clusters and Slurm (three 33 minute sessions,
    punctuated by two 10 minute breaks).  We will be exploring the
    use of all of these with some bioinformatic tasks.  Like indexing
    a genome. Mapping some reads, etc.
        - Homework: read from the beginning of the chapter
        to the end of section 8.2   [here](https://eriqande.github.io/eca-bioinf-handbook/chap-HPCC.html)
        - Learning about the cluster and your jobs: `sinfo` and `squeue`
        - Write a convenience bash function or two: `myjobs` and `alljobs`
        - Getting an interactive session on a compute node: `srun`
        - Environment modules on SEDNA: `module`
        - Submitting jobs to the queue: `sbatch`
        - Job arrays.
        - A useful script for orchestrating job arrays from a spreadsheet: `line-assign.sh`.  This will build off of
          what we learned the day before.
        - `scancel`: Help!  I need to kill that job (or all the jobs) that I just started!
        - `seff`: get information on resource use of a job I already ran.

* Day 3:
    - An introduction to Snakemake
        - Will somewhat follow [these slides](https://eriqande.github.io/con-gen-2022/slides/snake-slides.html#/section)
        - BUT, it will be modified to show use of SEDNA's environment modules
        and also to show use of a profile for dispatching jobs to Slurm from
        Snakemake.


