[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nmfs-bioinf-2022",
    "section": "",
    "text": "Welcome to the website for the National Marine Fisheries Service Linux, Slurm, and Bioinformatics training to be held virtually over three days:\n\nDay 1: Monday, October 17, 2022. 10 AM - 12 PM PDT\nDay 2: Tuesday, October 18, 2022. 10 AM - 12:30 PM PDT\nDay 3: Wednesday, October 19, 2022. 10 AM - 12 PM PDT\n\n\n\nThis course will be using the SEDNA high-performance computing cluster located at the Northwest Fisheries Science Center. This cluster (and, hence, this course) is only available to NMFS employees and affiliates. If you are a NMFS employee and you are interested in this course, please see here for information about how to get an account on the cluster.\nThis course is intended for people who have already had some exposure to Unix or Linux. You should be reasonably comfortable navigating around the Unix filesystem using the command line. For a refresher, please read this chapter from my online bioinformatics book.\nMy goal is to:\n\nteach the shell programming constructs and the text processing tricks that I find myself using all the time in my day-to-day work\nprovide an introduction to how to use SLURM to do cluster computing\nOn the last day, show how Snakemake works, and how it can be used on the SEDNA cluster to simplify your bioinformatics life.\n\n\n\n\n\nDay 1: Intro, Unix-review, shell programming, awk\n\nIntroduction to the SEDNA cluster (15 minutes Krista and Giles)\n\nCluster infrastructure and configuration.\nScientific software and the installation requests\n\nQuick Unix Review (25 minutes)\nShell Programming (50 Minutes)\nProcessing text files with awk (30 minutes)\n\nDay 2: A little bash stuff, then SLURM, SLURM, SLURM\n\nComputing clusters and Slurm (three 33 minute sessions, punctuated by two 10 minute breaks). We will be exploring the use of all of these with some bioinformatic tasks. Like indexing a genome. Mapping some reads, etc.\n\nHomework: read from the beginning of the chapter to the end of section 8.2 here\nLearning about the cluster and your jobs: sinfo and squeue\nWrite a convenience bash function or two: myjobs and alljobs\nGetting an interactive session on a compute node: srun\nEnvironment modules on SEDNA: module\nSubmitting jobs to the queue: sbatch\nJob arrays.\nA useful script for orchestrating job arrays from a spreadsheet: line-assign.sh. This will build off of what we learned the day before.\nscancel: Help! I need to kill that job (or all the jobs) that I just started!\nseff: get information on resource use of a job I already ran.\n\n\nDay 3:\n\nAn introduction to Snakemake\n\nWill somewhat follow these slides\nBUT, it will be modified to show use of SEDNA’s environment modules and also to show use of a profile for dispatching jobs to Slurm from Snakemake."
  },
  {
    "objectID": "quick-unix-review.html",
    "href": "quick-unix-review.html",
    "title": "1  Quick Unix Review",
    "section": "",
    "text": "We are going to take 15 or 20 minutes to do a whirlwind review of some crucial Unix topics"
  },
  {
    "objectID": "quick-unix-review.html#setting-up-our-workspace",
    "href": "quick-unix-review.html#setting-up-our-workspace",
    "title": "1  Quick Unix Review",
    "section": "1.1 Setting up our workspace",
    "text": "1.1 Setting up our workspace\n\nI have prepared a repository with a few different example data files that we be using.\nIt also contains all these notes.\nI want everyone to download it to their home directory and then cd into its playground directory, where we will be playing today and tomorrow.\n\nAfter logging onto SEDNA:\n\n\nPaste this into your shell\n\ncd ~\ngit clone https://github.com/eriqande/nmfs-bioinf-2022.git\ncd nmfs-bioinf-2022/playground\n\n\nThis is where our working directory will be for the next two days.\nUse the tree utility to see the files that we have to play with within this playground:\n\n\n\nType this command at your prompt\n\ntree\n\nThe data directory has a few things that we will be using for examples.\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ncd: change directories\ngit: run git subcommands, like clone with this. In the above case it clones the repository that is found at the GitHub URL.\ntree: Super cool “text-graphical” directory listing"
  },
  {
    "objectID": "quick-unix-review.html#a-motivating-example",
    "href": "quick-unix-review.html#a-motivating-example",
    "title": "1  Quick Unix Review",
    "section": "1.2 A motivating example",
    "text": "1.2 A motivating example\n\nThe data/samtools_stats directory has gzipped output from running the samtools stats program on 30 different samples.\nThis provides information about reads that have been mapped to a reference genome in a BAM file.\n\nTo see what those files look like:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\n\n\nHit the SPACE-bar to go down a screenful, the b key to go back up\nMost terminal emulators let you use up-arrow and down-arrow to go one line at a time, too.\nHit the q key to quit out of the less viewer.\n\nTo see it without lines wrapping all over the place try this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\n\nNow you can use the left and right arrows to see different parts of lines that are not wrapped on the screen.\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ngzip -cd: decompress gzipped file to stdout (this is what zcat does, but zcat is not portable).\nless: page view. Great to pipe output into. (SPACE-bar, b, q, down-arrow, up-arrow)\n\nless -S: option to not wrap lines. (left-arrow, right-arrow)\n\n\n\n\n\n\n1.2.1 (One of) Our Missions…\nIt is pretty typical that Bioinformatic outputs will be spread as small bits of information across multiple files.\nOne motivating example is summarizing the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped, in all 30 samples, in a table."
  },
  {
    "objectID": "quick-unix-review.html#the-anatomy-of-a-unix-command",
    "href": "quick-unix-review.html#the-anatomy-of-a-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.3 The anatomy of a Unix command",
    "text": "1.3 The anatomy of a Unix command\nNearly every line in a bash script, or every line you type when banging away at the Unix terminal is a command that has this structure:\ncommand options arguments\n\nThe command is the name of the command itself (like cd or less).\nThe options are often given:\n\nwith a dash plus a single character, like -l or -S or -a, -v, -z.\n\nIn most commands that are part of Unix, these single options can be combined, so -cd, is the same as -c -d.\n\nwith two dashes and a word, like --long or --version\nSometimes options take arguments, like --cores 20, but sometimes, they stand alone.\n\nWhen they stand alone they are sometimes called flags.\n\n\nThe arguments are typically file or paths.\n\n\n\n\n\n\n\nSelf-study question\n\n\n\nIdentify the command, options, and arguments in:\ntree -d ..\n\n# and\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz\n\n\n\n\n\n\n\n\nSelf-study answer\n\n\n\n\n\nFirst case:\n\ntree is the command\n-d is the option (print only directory names, not files)\n.. is the argument (one directory level up)\n\nSecond case:\n\ngzip is the command\nThe options are -c and -d, contracted into -cd\ndata/samtools_stats/s001_stats.tsv.gz is the argument\n\n\n\n\n\n1.3.1 What do all these options mean?\nEverything you need to know about any Unix command will typically be found with the man command. For example:\n\n\ntype this at your terminal\n\nman tree\n\n\nThat gives you more information than you will ever want to know.\nIt starts with a synopsis of the syntax, which can feel very intimidating.\n\n\n\n\n\n\n\nBonus Tips:\n\n\n\n\n\n\nMan uses the less viewer for presenting contents of the man pages.\nWhen you are viewing man pages, you can scroll down with SPACE-bar and up with b, and get out with q, just like in less\nTo search for patterns in the manual pages, you can type / then the string you want and then RETURN.\n\nWhen in pattern-searching mode, use n to go to the next occurrence, and N to the previous.\nIf searching for a single letter option try searching with [, ] afterward.\nFor example, to search for the -d flag you would type /, then -d[, ], then hit RETURN. Try it on the tree man page.\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nLearn about gzip\n\nUse man to read information about the gzip command\nFind information about the -c and the -d options.\n\nMaybe even search for those using the “slash-pattern” bonus tip from above.\n\n\nLearn about the ls command\n\nUse man to see information about the ls command, which lists directories and their contents\nFind out what the -R option does. Maybe even look for it using the Bonus Tip above.\nDo the same for the -Q option.\nLook at what those do by doing ls -RQ on your terminal.\n\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\n\n\nman gzip\nTo search for -c, type /-c + return. You might have better results with /-c[, ].\n\nuse n or N to go forward or backward through the occurrences of -c.\n\n\n\nYou would do man ls\nTo search for -R in the man pages, a good way to do it would be to type /-Q + RETURN, or maybe /-Q[, ] + RETURN."
  },
  {
    "objectID": "quick-unix-review.html#streams-and-redirection",
    "href": "quick-unix-review.html#streams-and-redirection",
    "title": "1  Quick Unix Review",
    "section": "1.4 Streams and redirection",
    "text": "1.4 Streams and redirection\n\nWhen you’ve executed the unix commands above, they have typically responded by writing text or data to the terminal screen.\nThe command is actually writing to a stream that is called stdout, which is short for “standard output.”\nIt turns out that, by default, the stdout stream gets written to the terminal.\n\nAha! But here is where it gets fun:\n\nYou can redirect the stdout stream to a file by using > or >> after the command, options, and arguments.\n\nFor example:\n\n\nPaste this into your terminal\n\nmkdir outputs\ntree -d .. > outputs/repo-tree.txt\n\nNow, you can use the less viewer to see what got into the file outputs/repo-tree.txt:\n\n\nType this at the terminal\n\nless outputs/repo-tree.txt\n\nAha! Instead of writing the output to the screen, it just puts it in the file outputs/repo-tree.txt, as we told it to.\n\n\n\n\n\n\nDanger!\n\n\n\nIf you redirect stdout into a file that already exists, the contents of that file will get erased!!!\nFor example, if you now do:\n\n\nPaste this into the shell\n\necho \"New content coming through...\" > outputs/repo-tree.txt\n\nThen you will no longer have the output of the tree command in the file outputs/repo-tree.txt. Check it out with the less command.\n\n\nIf you want to merely append stdout to an existing file, you can use >>. For example:\n\n\nPaste this into your terminal\n\necho \"Add this line\" >> outputs/repo-tree.txt\necho \"And then add another line\" >> outputs/repo-tree.txt\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\nmkdir: make a new directory\n\n(check out the -p option, which means “make any necessary parent directories and don’t complain if the directory already exists.”)\n\necho: print the argument (usually a string) to stdout."
  },
  {
    "objectID": "quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "href": "quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.5 Pipes: redirecting into another Unix command",
    "text": "1.5 Pipes: redirecting into another Unix command\nAs we have said, many Unix utilities take files as their arguments, and they operate on the contents of that file. They can also receive input from streams, and almost all Unix utilities are set up to accept input from the stream called stdin, which is short for standard input.\n\nThe most important way to pass the stdin stream to a Unix command is by piping the stdout from one command in as the stdin to the next command.\nThis uses the | which is called the “pipe”.\n\nWe have already used the pipe when we did:\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\nPipe syntax is pretty simple:\ncommand1 | command2\nmeans pipe the stdout output of command1 in as stdin input for command2."
  },
  {
    "objectID": "quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "href": "quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "title": "1  Quick Unix Review",
    "section": "1.6 stderr: The stream Unix uses to yell at you",
    "text": "1.6 stderr: The stream Unix uses to yell at you\n\nIf a Unix command fails, typically the program/command will bark at you to tell you why it failed. This can be very useful.\nThe stream it writes this information to is called stderr, which is short for standard error.\nSome bioinformatics programs write progess and log output to stderr.\n\nIf you are running a program non-interactively, it is extremely valuable and important to redirect stderr to a file, so you can come back later to see what went wrong, if your job failed.\n\nstderr is redirected with 2>.\nThink of the 2 as meaning that stderr is the second-most important stream, after stdout.\n\n\n\n\n\n\n\nBonus side comment:\n\n\n\n\n\nAs you might imagine, you could redirect stdout by using 1> instead of >, since stdout is stream #1.\n\n\n\nFor example, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. > outputs/repo-tree.txt\n\n\nAha! We get a warning note printed on the screen,\nBecause, stderr gets printed to the terminal by default.\nAlso outputs/repo-tree.txt has been overwritten and is now a file with nothing in it.\n\nSo, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. > outputs/repo-tree.txt 2>outputs/error.txt\n\nNow, look at the contents of both outputs/error.txt and outputs/repo-tree.txt:\n\n\nPaste this into your shell\n\nhead outputs/repo-tree.txt outputs/error.txt\n\n\n\n\n\n\n\nStream operators and commands that we just saw:\n\n\n\n\n\n\n> path/to/file: redirect stdout to file at path/to/file. This overwrites any file already at path/to/file.\n>> path/to/file: redirect stdout to append to file at path/to/file. If path/to/file does not exist, it creates it and then adds the contents of stdout to it.\n2> path/to/file: redirect stderr to the file at path/to/file.\n|: the uber-useful Unix pipe. (Just as an aside, when R finally got a similar construct—the %>% from the ‘magrittr’ package—it became much easier for Unixy people to enjoy coding in R).\nhead: print the first ten lines of a file to stdout. If multiple file arguments are given, they are separated by little ==> filename <== lines, which is super convenient if you want to look at the top of a lot of files.\n\nhead -n XX: print the first XX lines (instead of 10).\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nDecompress data/samtools_stats/s001_stats.tsv.gz onto stdout using the gzip -cd command and pipe the output into wc to count how many lines words, and characters are in the file.\nDo the same that you did above, but redirect the stdout to a file so.txt and stderr to a file se.txt in the current working directory.\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nThese could be done like this:\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc > so.txt 2>se.txt\n\nAs an interesting side note, this will only redirect stderr for the wc command into se.txt. If the first command fails, its stderr will to to the screen. Try this:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc > so.txt 2>se.txt\n\nAn interesting fact is that you can redirect stderr from the first command before the pipe. So, to redirict stderr for the gzip command into a file called ze.txt, we could do:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz 2>ze.txt | wc > so.txt 2>se.txt\n\nHave a look at the contents of ze.txt."
  },
  {
    "objectID": "shell-prog.html",
    "href": "shell-prog.html",
    "title": "2  Shell Programming",
    "section": "",
    "text": "The bash shell is actually an interpreted programming language.\nIt has variables, for loops, flow control, etc.\nHere, we survey just a few of the most useful constructs that let us to a lot of things, and particularly allow us to tackle repetitive tasks with grace."
  },
  {
    "objectID": "shell-prog.html#variables",
    "href": "shell-prog.html#variables",
    "title": "2  Shell Programming",
    "section": "2.1 Variables",
    "text": "2.1 Variables\nThe bash shell can store strings into variables. These variables can be accessed later.\n\n2.1.1 Assigning variable values\n\nbash is remarkably picky about how to assign a value to a variable.\nUse the equals sign with no spaces around it!\n\nHere, we can assign a value to a variable called PHONE:\n\n\nPaste this into your shell\n\nPHONE=974-222-4444\n\nIn this case the digits and dashes 974-222-4444 are treated as just a string of characters and assigned to the variable PHONE.\n\n\n2.1.2 Accessing variable values\n\nThe process of accessing the values stored in the variable is called “variable substitution.\nIt means: “Substitute the value for the variable where it appears on the command line.”\nIn many programming languages, you can just write a variable’s name and know that its value will be accessed, like in R:\n\nVariable <- 16\nsqrt(Variable)\n\nHowever, in bash, variable substitution is achieved by prepending $ to the variable’s name.\n\nWitness:\n\n\nPaste this into your shell\n\necho The value of PHONE is: $PHONE\n\nCool!\n\nRemember: if you make a substitution to the menu at a fancy restaurant, it is going to cost you some dollars. Same way when you make a variable substitution in bash: it costs you a dollar sign and you have to pay up front.\n\n\n\n2.1.3 Valid bash variable names\nThe bash shell demands that the names of variables:\n\nStart with _ (an underscore) or a letter\nThen include only _, letters, or numbers\n\n# good variable names\nMY_JOBS\n_NOW\nSTRING\ni\ni_2\ni2\n\n# cannot be variable names\n1_node\n4THIS\nBIG-VAR\nfile-name\nSh**t!"
  },
  {
    "objectID": "shell-prog.html#strings-with-spaces-etc-quoting.",
    "href": "shell-prog.html#strings-with-spaces-etc-quoting.",
    "title": "2  Shell Programming",
    "section": "2.2 Strings with spaces, etc: Quoting.",
    "text": "2.2 Strings with spaces, etc: Quoting.\n\nIf you want to assign a string to a variable that has spaces in it you can quote the string, which holds it together as one “unit.”\n\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWhen the bash shell is interpreting a line of input, it breaks it into chunks called tokens which are separated by white space (spaces and TABS). If you wrap a series of words in quotation marks, it turns them all into a single token.\n\n\n\nFor example:\n\n\nPaste this into your shell\n\nMandela_Quote=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\necho $Mandela_Quote"
  },
  {
    "objectID": "shell-prog.html#variable-substitution-and-vs",
    "href": "shell-prog.html#variable-substitution-and-vs",
    "title": "2  Shell Programming",
    "section": "2.3 Variable substitution and \" vs '",
    "text": "2.3 Variable substitution and \" vs '\nWe have two types of quotes:\n\nsingle quotes, like '\ndouble quotes, like \"\n\nThey both chunk their contents into a single unit, but they behave very differently with respect to variable substition.\n\nSingle Quotes: surly and strict, you can’t substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho 'Dessert tonight is $DESSERT'\n\n\nDouble Quotes: soft and friendly, you CAN substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho \"Dessert tonight is $DESSERT\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nAssign values to three shell variables, NAME, FOOD, and ACTIVITY, so that when you run the following command, it makes sense:\n\n\nAfter assigning values to the three variables, run this command\n\necho \"My name is $NAME. I like to eat $FOOD, and I enjoy $ACTIVITY.\" \n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nFor my case, I could put:\nNAME=Eric\nFOOD=\"steamed broccoli\"\nACTIVITY=\"inline skating long distances\""
  },
  {
    "objectID": "shell-prog.html#variables-are-not-just-to-be-echoed",
    "href": "shell-prog.html#variables-are-not-just-to-be-echoed",
    "title": "2  Shell Programming",
    "section": "2.4 Variables are not just to be echoed!",
    "text": "2.4 Variables are not just to be echoed!\nInvariably, when learning how to use shell variables, all the examples have you using echo to print the value of the variable. How boring and misleading.\nIt is important to understand that after a value gets substituted onto the command line, the shell goes right ahead and evaluates the resulting command line.\nSo, you can record shell variables that are command lines that do something, themselves, once they are run as a command line.\nFor example, here we make a variable whose value is the command to decompress a file to stdout:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz\"\n\nAnd now if you just substitute that variable onto the command line\n\n\nType this at the command line\n\n$MyComm\n\nthe uncompressed contents of the file data/samtools_stats/s001_stats.tsv.gz go zooming by on your screen.\n\n2.4.1 Some subtlety about evaluation of substituted values\nIf the value of the variable that is being evaluated includes pipes, redirections, or variable assignment statements, then if you just subtituting it into the command line, it won’t properly be evaluated as a command line in full. For example, if MyComm was trying to decompress the file and pipe it to less, it doesn’t work as expected:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\"\n\nAnd now if you just substitute that variable onto the command line the shell gets confused, because it doesn’t recognize the pipe as a pipe!\n\n\nType this at the command line\n\n$MyComm\n\nHowever, you can use the eval keyword before $MyComm to ensure that the shell recognizes that you intend for it to evaluate pipes, redirects, shell variable assignment, etc. in the substituted variable value as it normally would:\n\n\nType this at the command line\n\neval $MyComm\n\nWe will end up using this later.\nRemember, you can hit q to get out of the less page viewer."
  },
  {
    "objectID": "shell-prog.html#multiple-commands-on-one-line-with",
    "href": "shell-prog.html#multiple-commands-on-one-line-with",
    "title": "2  Shell Programming",
    "section": "2.5 Multiple commands on one line with ;",
    "text": "2.5 Multiple commands on one line with ;\n\nYou can put a ; after a command, and it will behave like a line ending—the shell will run that command, and then go to the next.\n\nExample:\necho \"Let us do this command and 2 others\"; echo \"here is number 2\"; echo \"and the third\"\nThis comes in handy."
  },
  {
    "objectID": "shell-prog.html#repetition",
    "href": "shell-prog.html#repetition",
    "title": "2  Shell Programming",
    "section": "2.6 Repetition",
    "text": "2.6 Repetition\nLet’s face it, bioinformatics, or any sort of data analysis or processing often involves doing the same thing to a number of different inputs.\nMost unix utilities are designed so that if you give it multiple inputs it will do the same thing to each and report the results in a way that is easy to understand.\nFor example, to see how many lines, words, and characters are in each Quarto (the successor to RMarkdown) document that I used to make this website, we can use wc on all the files with a .qmd extension that are one directory level above where we are currently:\n\n\nPaste this into your shell\n\nwc ../*.qmd\n\nThis is nice."
  },
  {
    "objectID": "shell-prog.html#for-loops",
    "href": "shell-prog.html#for-loops",
    "title": "2  Shell Programming",
    "section": "2.7 For loops",
    "text": "2.7 For loops\nSometimes, however, we have more complex operations to do, so we can’t just provide multiple files to a single Unix utility.\nFor example, let’s say we want to know how many lines are in each of the samtools stats files in the data/samtools_stats directory. We can’t use wc directly, because these files are gzipped, and the result we get won’t be equal to the number of lines, words, and characters in each file.\nFor repetition in these cases, bash has a for loop. Its syntax looks like this:\nfor VAR in thing1 thing2 ... thingN; do\n  one or more commands where the value of VAR is set to each of the N things in turn\ndone\nThe important “structural” parts of that are:\n\nthe for\nthe in\nthe semicolon after all the things\nthe do\nthe done\n\nHere is an example:\n\n\nPaste this into your terminal\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do \n  echo \"I like $LIKE.\"\ndone\n\nNote that this is written over multiple lines, but we can substitute ; for the ends of statements and put it all on one line. (Useful if we are just hacking away on the command line…)\n\n\nThis does the same as the above one\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do echo \"I like $LIKE.\"; done\n\nNote! Don’t put a semicolon after do.\n\n\n\n\n\n\nSelf study\n\n\n\nWe want to give the number of text lines, words, and characters from all the samtools_stats files.\nPrep: Here is a command that prints the name of each file.\n\n\nPaste this into the terminal\n\nfor FILE in data/samtools_stats/*.gz; do echo $FILE; done\n\nTask: I have added the -n option to the echo command which makes it not print a line ending. Your task is to replace YOUR_STUFF_HERE with an appropriate shell command to decompress each file and then print the number of lines, words, and characters in it:\n\n\nPaste this, edit YOUR_STUFF_HERE, and run it\n\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; YOUR_STUFF_HERE; done\n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nYour edited command line should look like this:\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; gzip -cd $FILE | wc; done"
  },
  {
    "objectID": "shell-prog.html#redirect-stdout-from-the-done",
    "href": "shell-prog.html#redirect-stdout-from-the-done",
    "title": "2  Shell Programming",
    "section": "2.8 Redirect stdout from the done",
    "text": "2.8 Redirect stdout from the done\nHere is something that is not always obvious: you can redirect or pipe the stdout of the whole for loop by using > or | immediately after the done keyword.\nUsing the example from the self study above:\n\n\nPaste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do \n  echo -n $FILE; gzip -cd $FILE | wc; \ndone > word_counts.txt\n\nNow look at what is in word_counts.txt."
  },
  {
    "objectID": "shell-prog.html#a-few-useful-topics-rapidly",
    "href": "shell-prog.html#a-few-useful-topics-rapidly",
    "title": "2  Shell Programming",
    "section": "2.9 A few useful topics, rapidly",
    "text": "2.9 A few useful topics, rapidly\n\n2.9.1 basename\n\nIf you have a path name to a file this_dir/that_dir/my_file3.txt, but you want to have the string for just the file name, my_file3.txt, you can use the basename command:\n\n\n\nTry these\n\nbasename this_dir/that_dir/my_file3.txt\nbasename ~/Documents/git-repos/CKMRpop/R/plot_conn_comps.R\n\n\n\n2.9.2 Capture stdout into a token to put on the command line\nThis is a pretty cool one, and is really nice if you want to capture a bit of output for use at a later time.\nBasically, if you run a command inside parentheses that are immediately preceded by a $, like $(command), then the stdout output of command gets put onto the command line as a single token.\nObserve:\n\n\nPaste this into your terminal\n\nSTART_TIME=$(date)\nsleep 3\nSTOP_TIME=$(date)\necho \"We started at $START_TIME, and finished at $STOP_TIME, and it is now $(date)\"\n\nOr even:\n\n\nPaste this into your terminal\n\nWCOUT=$(gzip -cd data/samtools_stats/s016_stats.tsv.gz | wc)\necho $WCOUT\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ndate: prints the current time and date to stdout.\nsleep: causes the shell to pause for however seconds you tell it to, like sleep 3 for three seconds, sleep 180 for three minutes."
  },
  {
    "objectID": "shell-prog.html#fancier-variable-substitution",
    "href": "shell-prog.html#fancier-variable-substitution",
    "title": "2  Shell Programming",
    "section": "2.10 Fancier variable substitution",
    "text": "2.10 Fancier variable substitution\n\n2.10.1 Wrap it in curly braces ${VAR}\nEspecially if you want to substitute a variable into a string adjacent to a letter or number or underscore, you can wrap it in curly braces.\n\n\nTry this\n\nsample=001\n# this works:\necho \"The sequences are in the file called ${sample}_seqs.fq.gz\"\n\n# this does not work the way you want it to\necho \"The sequences are in the file called $sample_seqs.fq.gz\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nWhy do you think the second echo line above produced the output that it did?\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nSince _ is a valid character for a variable name, $sample_seqs.fq.gz gets broken up by the shell as $sample_seqs plus .fq.gz, and there is no variable named sample_seqs, so the shell just substitutes an empty string into $sample_seqs.\n\n\n\n\n\n2.10.2 Variable modification while substituting it\nbash has a whole lot of tricky syntaxes for manipulating variable values when substituting them onto the command line.\nThe one I use more than any other is ${VAR/pattern/replacement}. This looks for a text pattern pattern in the variable VAR and replaces it with the text replacement.\nHere are some examples.\n\n\nFancy substitution fun. Paste into your terminal.\n\nfile=myfile.eps\necho ${file/eps/pdf}\n\n# or maybe you want to get the sample name, s007\n# out of a file path like data/samtools_stats/s007_stats.tsv.gz\npath=data/samtools_stats/s007_stats.tsv.gz\necho $(basename ${path/_stats.tsv.gz/})\n\nWhoa! On that last one we nested a ${//} inside a $()!\n\n\n2.10.3 Grouping multiple commands with (...)\n\nSometimes it is convenient lump a number of commands together into a group.\nThe main reason I do this is to capture stdout or stderr from all of them into a single file with one redirect (as opposed to redirecting (>) output from the first command, and then redirect-appending (>>) output from successive commands to the same place).\nWhen you wrap a series of commands in a pair of parentheses, they get executed as a group and you can redirect that from the right side of the last parenthesis:\n\n\n\nPaste this into your terminal\n\n(\n  echo \"This\"\n  echo \"that\"\n  echo \"and the\"\n  echo \"other\"\n) > group_it.txt\n\nCheck out the result with cat group_it.txt.\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWith grouping parentheses, you can also redirect stderr to a single place even if there are pipes involved. Comparing to the Self-Study answer at the end of the Quick Unix Review session:\n\n\nA stderr example\n\n(gzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc) > so.txt 2>se.txt\n\nEven though the error happened with the gzip command, the error message gets relayed through to be redirected into se.txt."
  },
  {
    "objectID": "shell-prog.html#leaving-here-for-now",
    "href": "shell-prog.html#leaving-here-for-now",
    "title": "2  Shell Programming",
    "section": "2.11 Leaving here for now…",
    "text": "2.11 Leaving here for now…\nYou might be thinking, “Wow, could I use a for loop or something like it in bash to cycle over the lines of a text file to process each line in turn?”\nThe answer, is, “You can, but bash is not always the best tool for processing text files…especially if they are large.”\nThere is a Unix utility called awk that is much better for that.\nThat is where we are heading next."
  },
  {
    "objectID": "awk-intro.html",
    "href": "awk-intro.html",
    "title": "3  A Brief awk Intro",
    "section": "",
    "text": "awk is a lighweight, completely awesome, little scripting language that is perfect for ripping through large text files with a minimal memory footprint.\nWe will only scratch the surface of it, here, but I hope it encourages awk newcomers to explore it more.\nawk can make many tasks on the command line simple, fast, and fun.\nFor a more detailed introduction, you might find this chapter of my Bioinformatics Handbook of some use. (And there are many other resources online, as well)."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "eca-setup-notes.html",
    "href": "eca-setup-notes.html",
    "title": "5  Notes on Eric’s Setup Process",
    "section": "",
    "text": "These are just notes to myself. People shouldn’t follow these steps…\nI copied my existing ones into ~/BACKUP_PROFILES, then put the default ones into place:\n(base) [sedna: ~]--% cp /etc/skel/.bashrc ./\n(base) [sedna: ~]--% cp /etc/skel/.bash_profile ./\nThen I logged out and back in. Then I ran this line:\neval \"$(/opt/bioinformatics/miniconda3/bin/conda shell.bash hook)\"\nAnd I noted that in this base environment, mamba is available.\nI also listed all the environments, and I have a boatload in my home directory, but the main one that everyone is going to need is:\n/opt/bioinformatics/miniconda3/envs/snakemake-7.7.0\nSo, after that, I added the eval \"$(/opt/bioinformatics/miniconda3/bin/conda shell.bash hook)\" line to my .bashrc and then logged out and back in.\nThen, I tried this:L\n# in: /home/eanderson/Documents/git-repos/nmfs-bioinf-2022/Snakemake-Example\nconda activate /opt/bioinformatics/miniconda3/envs/snakemake-7.7.0\nrm -rf results resources/genome.fasta.* resources/genome.dict\n\n# then\nsnakemake -np --cores 20 --use-envmodules results/vcf/all.vcf\n\n# that knew just what to do.  And I tried that on a node with 20 cores\nsnakemake  --cores 20 --use-envmodules results/vcf/all.vcf"
  },
  {
    "objectID": "awk-intro.html#what-awk-is-designed-to-do",
    "href": "awk-intro.html#what-awk-is-designed-to-do",
    "title": "3  A Brief awk Intro",
    "section": "3.1 What awk is designed to do",
    "text": "3.1 What awk is designed to do\nawk is a utility that:\n\nTakes text input from a file or from stdin\nIt automatically goes line-by-line. Treating each line of text as a separate unit.\nWhen it is focused on a line of text, it breaks it into fields which you can think of as columns.\nBy default, fields, are broken up on whitespace (spaces and TABs), with multiple spaces or TABs being treated as a single delimiter.\nYou can specify the delimiter.\n\nLet’s look through a file together. I will do:\n\n\nYou can do this if you want\n\ngzip -cd data/samtools_stats/s004_stats.tsv.gz | less -S\n\nand we will discuss how awk sees such a file.\n\n3.1.1 Naming fields\nOnce awk has read a line of text into memory, and split it into fields, you can access the value of those fields with special names:\n\n$1 refers to the first field (column)\n$2 refers to the second column.\n\nIf you are beyond the ninth column, the number has to be wrapped in parentheses:\n\n$(13) refers to the thirteenth column\n\n\n\n3.1.2 So what?\nSo far, this doesn’t seem very exciting, but now we learn that…\n\nYou can tell awk the sorts of lines you want it to pause at and then do some action upon it.\n\nYou tell awk which lines you want to perform an action on by matching them with a logical statement called a pattern in awk parlance.\n\nThis turns out to be incredibly powerful.\n\n\n3.1.3 awk syntax on the command line\nThe syntax for running awk with a script on the command line is like this:\n\n\nDon't try running this. It is just for explanation.\n\nawk '\n  pattern1  {action1}\n  pattern2  {action2}\n' file\n\nWhere file is the name of the file you want to process.\nOr, if you are piping text into awk from some command named cmd it would look like this:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk '\n  pattern1  {action1}\n  pattern2  {action2}\n'\n\nNote that you can pass options (like -v or -F) to awk. They go right after the awk and before the first single quotation mark.\nAlso, the carriage returns inside the single quotation marks are only there for easy reading. You could also write last example as:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk 'pattern1  {action1} pattern2  {action2}'\n\n…which is great if you are hacking on the command line, but once you have a lot of pattern-action pairs, it gets harder to read."
  },
  {
    "objectID": "awk-intro.html#awks-philosophy-and-basic-syntax",
    "href": "awk-intro.html#awks-philosophy-and-basic-syntax",
    "title": "3  A Brief awk Intro",
    "section": "3.1 awk’s philosophy and basic syntax",
    "text": "3.1 awk’s philosophy and basic syntax\nawk is a utility that:\n\nTakes text input from a file or from stdin\nIt automatically goes line-by-line. Treating each line of text as a separate unit.\nWhen it is focused on a line of text, it breaks it into fields which you can think of as columns.\nBy default, fields, are broken up on whitespace (spaces and TABs), with multiple spaces or TABs being treated as a single delimiter.\nYou can specify the delimiter.\n\nLet’s look through a file together. I will do:\n\n\nYou can do this if you want\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\nand we will discuss how awk sees such a file.\n\n3.1.1 Naming fields\nOnce awk has read a line of text into memory, and split it into fields, you can access the value of those fields with special names:\n\n$1 refers to the first field (column)\n$2 refers to the second column.\n\nIf you are beyond the ninth column, the number has to be wrapped in parentheses:\n\n$(13) refers to the thirteenth column\n\n\n\n3.1.2 So what?\nSo far, this doesn’t seem very exciting, but now we learn that…\n\nYou can tell awk the sorts of lines you want it to pause at and then do some action upon it.\n\nYou tell awk which lines you want to perform an action on by matching them with a logical statement called a pattern in awk parlance.\n\nThis turns out to be incredibly powerful.\n\n\n3.1.3 awk syntax on the command line\nThe syntax for running awk with a script on the command line is like this:\n\n\nDon't try running this. It is just for explanation.\n\nawk '\n  pattern1  {action1}\n  pattern2  {action2}\n' file\n\nWhere file is the name of the file you want to process.\nOr, if you are piping text into awk from some command named cmd it would look like this:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk '\n  pattern1  {action1}\n  pattern2  {action2}\n'\n\nNote that you can pass options (like -v or -F) to awk. They go right after the awk and before the first single quotation mark.\nAlso, the carriage returns inside the single quotation marks are only there for easy reading. You could also write last example as:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk 'pattern1  {action1} pattern2  {action2}'\n\n…which is great if you are hacking on the command line, but once you have a lot of pattern-action pairs, it gets harder to read."
  },
  {
    "objectID": "awk-intro.html#enough-talking-lets-start-doing",
    "href": "awk-intro.html#enough-talking-lets-start-doing",
    "title": "3  A Brief awk Intro",
    "section": "3.2 Enough talking, let’s start doing",
    "text": "3.2 Enough talking, let’s start doing\nAll of this will make more sense with a few examples.\n\n3.2.1 Print all the lines in which the first field is SN\nFor our first foray, let’s just pick out and print a subset of lines from our samtools stats file:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '$1==\"SN\" {print}'\n\nThat is cool.\nHow about if we wanted to pick out just a few particular lines from there?\nWell, we can also match lines by regular expression (which you can think of as a very fancy form of Unix word-searching.)\n\n\n3.2.2 Printing the lines we are interested in\nLet’s say that we want information on the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped using the (cigar) criterion.\nWe can see those lines in there. And we can target them by matching strings associated with them. The awk syntax puts these regular expressions in the pattern between forward slashes.\nSo, we want to match lines that have the first field equal to SN and also match other strings. We do that like this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print}\n  $1==\"SN\" && /reads properly paired:/ {print}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print}\n'\n\nBig Note regular expressions are lovely and wonderful, but occasionally frustrating. In awk’s case, parentheses have special meanings in the regular expressions, so we have to precede each one in the pattern with a backslash.\nRegular expressions are a bit beyond the scope of what we will be talking about today (entire books are devoted to the topic) but I encourage everyone to learn about them.\n\n\n3.2.3 Printing just the values we are interested in\nThat is nice, but remember, we really just want to put those three values we are interested in into a table of sorts.\nSo, how do we print just the values?\nUse the fields! Count columns for each line and then print just that field:\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print $4}\n  $1==\"SN\" && /reads properly paired:/ {print $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print $5}\n'\n\n\n\n3.2.4 Use variables inside awk\nWe can also assign values to variables inside awk.\nThis lets us store values and then print them all on one line at the end. The special pattern END gives us a block to put actions we want to do at the very end.\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print rm, rpp, bmc}\n'\n\n\n\n3.2.5 That’s great. Can we add the sample name in there?\nYes! We can pass variables from the command line to inside awk with a -v var=value syntax.\nTo do this, we use some shell code that we learned earlier!\n\n\nStudy this, then paste this into your terminal\n\nFILE=data/samtools_stats/s001_stats.tsv.gz\ngzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print samp, rm, rpp, bmc}\n'\n\n\n\n3.2.6 OMG! Do you seen where we are going with this?\nWe can now take that whole thing and imbed it within a bash for loop cycling over values of FILE and get the table talked about wanting in our Motivating Example when we started.\n\n\nStudy this, then paste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nYowzers! That is pretty quick, and it sure beats opening each file, copying the values we want, and then pasting them into a spreadsheet."
  },
  {
    "objectID": "awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "href": "awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "title": "3  A Brief awk Intro",
    "section": "3.3 Another example: the distribution of mapping qualities",
    "text": "3.3 Another example: the distribution of mapping qualities\nHere is a fun awk example that just came up a couple of days ago for me.\n\nA colleague was telling me that she has started filtering her whole-genome sequencing BAM files so that she does not use any reads that map with a mapping quality less than 30.\nThe hope is that this will lessen batch effects.\nQuestions:\n\nWhat is the distribution of mapping quality scores in my own data\nIf we imposed such a filter, how many reads would we discard?\n\n\nIt turns out that none of the samtools programs stat, idxstats, or flagstats provide that distribution.\nThere are some other more obscure software packages that provide it, but also a lot of convoluted python code on the BioStars website for doing it.\nHa! It’s quick and easy with awk! And a great demonstration of awk’s associative arrays.\n\n3.3.1 Let’s look at an example bam file\nWe have an example bam file in the repository at data/bam/s0001.bam.\nIt only has only about 25,000 read in it so that it isn’t too large.\nLet’s have a look at it with:\n\n\nPaste this into your terminal\n\nmodule load bio/samtools\nsamtools view bam/s001.bam | less -S\n\nThe module load bio/samtools line gives us access to the samtools program, which we need for turning BAM files into text-based SAM files that we can use. Once we have given it in our shell, we have that access until we close the shell. Much more on that tomorrow!\nThe mapping quality is in the 5th field. It is a number that ranges from 0 to 60. We can count up how many times each of those numbers occurs using awk.\n\n\nHere it is all on one line as I wrote it\n\nsamtools view bam/s001.bam | awk 'BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} {n[$5]++} END {for(i in n) tot+=n[i];  for(i=60;i>=0;i--) {sum+=n[i]; print i, n[i], sum/tot;}}'\n\nAnd\n\n\nHere it is broken across lines. Paste that in your shell.\n\nsamtools view bam/s001.bam | awk '\n  BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} \n  {n[$5]++} \n  END {for(i in n) tot+=n[i];  for(i=60;i>=0;i--) {sum+=n[i]; print i,n[i],sum/tot;}}\n'\n\nIt’s really compact and requires very little memory to do this. (You couldn’t read a whole BAM file into R and hope to deal with it)."
  },
  {
    "objectID": "awk-intro.html#wrap-up",
    "href": "awk-intro.html#wrap-up",
    "title": "3  A Brief awk Intro",
    "section": "3.4 Wrap-Up",
    "text": "3.4 Wrap-Up\nThat was just a brief whirlwind tour of how one can use bash and awk together to automate tasks that come up in"
  }
]