[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nmfs-bioinf-2022",
    "section": "",
    "text": "Welcome to the website for the National Marine Fisheries Service Linux, Slurm, and Bioinformatics training to be held virtually over three days:\n\nDay 1: Monday, October 17, 2022. 10 AM - 12 PM PDT\nDay 2: Tuesday, October 18, 2022. 10 AM - 12:30 PM PDT\nDay 3: Wednesday, October 19, 2022. 10 AM - 12 PM PDT\n\n\n\nThis course will be using the Sedna high-performance computing cluster located at the Northwest Fisheries Science Center. This cluster (and, hence, this course) is only available to NMFS employees and affiliates. If you are a NMFS employee and you are interested in this course, please see here for information about how to get an account on the cluster.\nThis course is intended for people who have already had some exposure to Unix or Linux. You should be reasonably comfortable navigating around the Unix filesystem using the command line. For a refresher, please read this chapter from my online bioinformatics book.\nMy goal is to:\n\nteach the shell programming constructs and the text processing tricks that I find myself using all the time in my day-to-day work\nprovide an introduction to how to use SLURM to do cluster computing\nOn the last day, show how Snakemake works, and how it can be used on the Sedna cluster to simplify your bioinformatics life.\n\n\n\n\n\nDay 1: Intro, Unix-review, shell programming, awk\n\nIntroduction to the Sedna cluster (15 minutes Krista and Giles)\n\nCluster infrastructure and configuration.\nScientific software and the installation requests\n\nQuick Unix Review (25 minutes)\nShell Programming (50 Minutes)\nProcessing text files with awk (30 minutes)\n\nDay 2: A little bash stuff, then SLURM, SLURM, SLURM\n\nShell scripts and bash functions\nComputing clusters and Slurm (three 33 minute sessions, punctuated by two 10 minute breaks). We will be exploring the use of all of these with some bioinformatic tasks. Like indexing a genome. Mapping some reads, etc.\n\nHomework: read from the beginning of the chapter to the end of section 8.2 here\nLearning about the cluster and your jobs: sinfo and squeue\nWrite a convenience bash function or two: myjobs and alljobs\nGetting an interactive session on a compute node: srun\nEnvironment modules on Sedna: module\nSubmitting jobs to the queue: sbatch\nJob arrays.\nA useful script for orchestrating job arrays from a spreadsheet: line-assign.sh. This will build off of what we learned the day before.\nscancel: Help! I need to kill that job (or all the jobs) that I just started!\nseff: get information on resource use of a job I already ran.\n\n\nDay 3:\n\nAn introduction to Snakemake\n\nWill somewhat follow these slides\nBUT, it will be modified to show use of Sedna’s environment modules and also to show use of a profile for dispatching jobs to Slurm from Snakemake."
  },
  {
    "objectID": "quick-unix-review.html",
    "href": "quick-unix-review.html",
    "title": "1  Quick Unix Review",
    "section": "",
    "text": "We are going to take 15 or 20 minutes to do a whirlwind review of some crucial Unix topics"
  },
  {
    "objectID": "quick-unix-review.html#what-is-bash",
    "href": "quick-unix-review.html#what-is-bash",
    "title": "1  Quick Unix Review",
    "section": "1.1 What is bash?",
    "text": "1.1 What is bash?\nWe start by acknowledging that there are many different flavors of Unix and Linux. I will refer to them all simply as Unix or unix.\nAlso, there are a number of different shells for Unix. The shell is the part that interprets commands.\nWe will be talking about the bash shell. This is the default shell on Sedna, and it is also the most popular shell for bioinformatics.\nBash stands for “Bourne-again shell”. It is an update to an earlier shell called the Bourne shell."
  },
  {
    "objectID": "quick-unix-review.html#setting-up-our-workspace",
    "href": "quick-unix-review.html#setting-up-our-workspace",
    "title": "1  Quick Unix Review",
    "section": "1.2 Setting up our workspace",
    "text": "1.2 Setting up our workspace\n\nI have prepared a repository with a few different example data files that we be using.\nIt also contains all these notes.\nI want everyone to download it to their home directory and then cd into its playground directory, where we will be playing today and tomorrow.\n\nAfter logging onto Sedna:\n\n\nPaste this into your shell\n\ncd ~\ngit clone https://github.com/eriqande/nmfs-bioinf-2022.git\ncd nmfs-bioinf-2022/playground\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen we say “Paste this into your shell” or “Type this at your command prompt” we also implicitly mean “Hit RETURN afterward.”\n\n\n\nThis is where our working directory will be for the next two days.\nUse the tree utility to see the files that we have to play with within this playground:\n\n\n\nType this command at your prompt\n\ntree\n\nThe data directory has a few things that we will be using for examples.\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ncd: change directories\ngit: run git subcommands, like clone with this. In the above case it clones the repository that is found at the GitHub URL.\ntree: Super cool “text-graphical” directory listing"
  },
  {
    "objectID": "quick-unix-review.html#a-motivating-example",
    "href": "quick-unix-review.html#a-motivating-example",
    "title": "1  Quick Unix Review",
    "section": "1.3 A motivating example",
    "text": "1.3 A motivating example\n\nThe data/samtools_stats directory has gzipped output from running the samtools stats program on 30 different samples.\nThis provides information about reads that have been mapped to a reference genome in a BAM file.\n\nTo see what those files look like:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\n\n\nHit the SPACE-bar to go down a screenful, the b key to go back up\nMost terminal emulators let you use up-arrow and down-arrow to go one line at a time, too.\nHit the q key to quit out of the less viewer.\n\nTo see it without lines wrapping all over the place try this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\n\nNow you can use the left and right arrows to see different parts of lines that are not wrapped on the screen.\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ngzip -cd: decompress gzipped file to stdout (this is what zcat does, but zcat is not portable).\nless: page view. Great to pipe output into. (SPACE-bar, b, q, down-arrow, up-arrow)\n\nless -S: option to not wrap lines. (left-arrow, right-arrow)\n\n\n\n\n\n\n1.3.1 (One of) Our Missions…\nIt is pretty typical that Bioinformatic outputs will be spread as small bits of information across multiple files.\nOne motivating example is summarizing the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped, in all 30 samples, in a table."
  },
  {
    "objectID": "quick-unix-review.html#the-anatomy-of-a-unix-command",
    "href": "quick-unix-review.html#the-anatomy-of-a-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.4 The anatomy of a Unix command",
    "text": "1.4 The anatomy of a Unix command\nNearly every line in a bash script, or every line you type when banging away at the Unix terminal is a command that has this structure:\ncommand options arguments\n\nThe command is the name of the command itself (like cd or less).\nThe options are often given:\n\nwith a dash plus a single character, like -l or -S or -a, -v, -z.\n\nIn most commands that are part of Unix, these single options can be combined, so -cd, is the same as -c -d.\n\nwith two dashes and a word, like --long or --version\nSometimes options take arguments, like --cores 20, but sometimes, they stand alone.\n\nWhen they stand alone they are sometimes called flags.\n\n\nThe arguments are typically file or paths.\n\n\n\n\n\n\n\nSelf-study question\n\n\n\nIdentify the command, options, and arguments in:\ntree -d ..\n\n# and\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz\n\n\n\n\n\n\n\n\nSelf-study answer\n\n\n\n\n\nFirst case:\n\ntree is the command\n-d is the option (print only directory names, not files)\n.. is the argument (one directory level up)\n\nSecond case:\n\ngzip is the command\nThe options are -c and -d, contracted into -cd\ndata/samtools_stats/s001_stats.tsv.gz is the argument\n\n\n\n\n\n1.4.1 What do all these options mean?\nEverything you need to know about any Unix command will typically be found with the man command. For example:\n\n\ntype this at your terminal\n\nman tree\n\n\nThat gives you more information than you will ever want to know.\nIt starts with a synopsis of the syntax, which can feel very intimidating.\n\n\n\n\n\n\n\nBonus Tips:\n\n\n\n\n\n\nMan uses the less viewer for presenting contents of the man pages.\nWhen you are viewing man pages, you can scroll down with SPACE-bar and up with b, and get out with q, just like in less\nTo search for patterns in the manual pages, you can type / then the string you want and then RETURN.\n\nWhen in pattern-searching mode, use n to go to the next occurrence, and N to the previous.\nIf searching for a single letter option try searching with [, ] afterward.\nFor example, to search for the -d flag you would type /, then -d[, ], then hit RETURN. Try it on the tree man page.\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nLearn about gzip\n\nUse man to read information about the gzip command\nFind information about the -c and the -d options.\n\nMaybe even search for those using the “slash-pattern” bonus tip from above.\n\n\nLearn about the ls command\n\nUse man to see information about the ls command, which lists directories and their contents\nFind out what the -R option does. Maybe even look for it using the Bonus Tip above.\nDo the same for the -Q option.\nLook at what those do by doing ls -RQ on your terminal.\n\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\n\n\nman gzip\nTo search for -c, type /-c + return. You might have better results with /-c[, ].\n\nuse n or N to go forward or backward through the occurrences of -c.\n\n\n\nYou would do man ls\nTo search for -R in the man pages, a good way to do it would be to type /-Q + RETURN, or maybe /-Q[, ] + RETURN."
  },
  {
    "objectID": "quick-unix-review.html#streams-and-redirection",
    "href": "quick-unix-review.html#streams-and-redirection",
    "title": "1  Quick Unix Review",
    "section": "1.5 Streams and redirection",
    "text": "1.5 Streams and redirection\n\nWhen you’ve executed the unix commands above, they have typically responded by writing text or data to the terminal screen.\nThe command is actually writing to a stream that is called stdout, which is short for “standard output.”\nIt turns out that, by default, the stdout stream gets written to the terminal.\n\nAha! But here is where it gets fun:\n\nYou can redirect the stdout stream to a file by using > or >> after the command, options, and arguments.\n\nFor example:\n\n\nPaste this into your terminal\n\nmkdir outputs\ntree -d .. > outputs/repo-tree.txt\n\nNow, you can use the less viewer to see what got into the file outputs/repo-tree.txt:\n\n\nType this at the terminal\n\nless outputs/repo-tree.txt\n\nAha! Instead of writing the output to the screen, it just puts it in the file outputs/repo-tree.txt, as we told it to.\n\n\n\n\n\n\nDanger!\n\n\n\nIf you redirect stdout into a file that already exists, the contents of that file will get erased!!!\nFor example, if you now do:\n\n\nPaste this into the shell\n\necho \"New content coming through...\" > outputs/repo-tree.txt\n\nThen you will no longer have the output of the tree command in the file outputs/repo-tree.txt. Check it out with the less command.\n\n\nIf you want to merely append stdout to an existing file, you can use >>. For example:\n\n\nPaste this into your terminal\n\necho \"Add this line\" >> outputs/repo-tree.txt\necho \"And then add another line\" >> outputs/repo-tree.txt\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\nmkdir: make a new directory\n\n(check out the -p option, which means “make any necessary parent directories and don’t complain if the directory already exists.”)\n\necho: print the argument (usually a string) to stdout."
  },
  {
    "objectID": "quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "href": "quick-unix-review.html#pipes-redirecting-into-another-unix-command",
    "title": "1  Quick Unix Review",
    "section": "1.6 Pipes: redirecting into another Unix command",
    "text": "1.6 Pipes: redirecting into another Unix command\nAs we have said, many Unix utilities take files as their arguments, and they operate on the contents of that file. They can also receive input from streams, and almost all Unix utilities are set up to accept input from the stream called stdin, which is short for standard input.\n\nThe most important way to pass the stdin stream to a Unix command is by piping the stdout from one command in as the stdin to the next command.\nThis uses the | which is called the “pipe”.\n\nWe have already used the pipe when we did:\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less\nPipe syntax is pretty simple:\ncommand1 | command2\nmeans pipe the stdout output of command1 in as stdin input for command2."
  },
  {
    "objectID": "quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "href": "quick-unix-review.html#stderr-the-stream-unix-uses-to-yell-at-you",
    "title": "1  Quick Unix Review",
    "section": "1.7 stderr: The stream Unix uses to yell at you",
    "text": "1.7 stderr: The stream Unix uses to yell at you\n\nIf a Unix command fails, typically the program/command will bark at you to tell you why it failed. This can be very useful.\nThe stream it writes this information to is called stderr, which is short for standard error.\nSome bioinformatics programs write progess and log output to stderr, in addition to actual error messages.\n\nIf you are running a program non-interactively, it is extremely valuable and important to redirect stderr to a file, so you can come back later to see what went wrong, if your job failed.\n\nstderr is redirected with 2>.\nThink of the 2 as meaning that stderr is the second-most important stream, after stdout.\n\n\n\n\n\n\n\nBonus side comment:\n\n\n\n\n\nAs you might imagine, you could redirect stdout by using 1> instead of >, since stdout is stream #1.\n\n\n\nFor example, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. > outputs/repo-tree.txt\n\n\nAha! We get a warning note printed on the screen,\nBecause, stderr gets printed to the terminal by default.\nAlso outputs/repo-tree.txt has been overwritten and is now a file with nothing in it.\n\nSo, try this:\n\n\nPaste this into your shell\n\ntree -d --not-a-real-option .. > outputs/repo-tree.txt 2>outputs/error.txt\n\nNow, look at the contents of both outputs/error.txt and outputs/repo-tree.txt:\n\n\nPaste this into your shell\n\nhead outputs/repo-tree.txt outputs/error.txt\n\n\n\n\n\n\n\nStream operators and commands that we just saw:\n\n\n\n\n\n\n> path/to/file: redirect stdout to file at path/to/file. This overwrites any file already at path/to/file.\n>> path/to/file: redirect stdout to append to file at path/to/file. If path/to/file does not exist, it creates it and then adds the contents of stdout to it.\n2> path/to/file: redirect stderr to the file at path/to/file.\n|: the uber-useful Unix pipe. (Just as an aside, when R finally got a similar construct—the %>% from the ‘magrittr’ package—it became much easier for Unixy people to enjoy coding in R).\nhead: print the first ten lines of a file to stdout. If multiple file arguments are given, they are separated by little ==> filename <== lines, which is super convenient if you want to look at the top of a lot of files.\n\nhead -n XX: print the first XX lines (instead of 10).\n\n\n\n\n\n\n\n\n\n\n\nSelf-study questions\n\n\n\n\nDecompress data/samtools_stats/s001_stats.tsv.gz onto stdout using the gzip -cd command and pipe the output into wc to count how many lines words, and characters are in the file.\nDo the same that you did above, but redirect the stdout to a file so.txt and stderr to a file se.txt in the current working directory.\n\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nThese could be done like this:\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | wc > so.txt 2>se.txt\n\nAs an interesting side note, this will only redirect stderr for the wc command into se.txt. If the first command fails, its stderr will to to the screen. Try this:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc > so.txt 2>se.txt\n\nAn interesting fact is that you can redirect stderr from the first command before the pipe. So, to redirect stderr for the gzip command into a file called ze.txt, we could do:\n\n\nPaste this into your shell\n\ngzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz 2>ze.txt | wc > so.txt 2>se.txt\n\nHave a look at the contents of ze.txt."
  },
  {
    "objectID": "shell-prog.html",
    "href": "shell-prog.html",
    "title": "2  Shell Programming",
    "section": "",
    "text": "The bash shell is actually an interpreted programming language.\nIt has variables, for loops, flow control, etc.\nHere, we survey just a few of the most useful constructs that let us to a lot of things, and particularly allow us to tackle repetitive tasks with grace."
  },
  {
    "objectID": "shell-prog.html#variables",
    "href": "shell-prog.html#variables",
    "title": "2  Shell Programming",
    "section": "2.1 Variables",
    "text": "2.1 Variables\nThe bash shell can store strings into variables. These variables can be accessed later.\n\n2.1.1 Assigning variable values\n\nbash is remarkably picky about how to assign a value to a variable.\nUse the equals sign with no spaces around it!\n\nHere, we can assign a value to a variable called PHONE:\n\n\nPaste this into your shell\n\nPHONE=974-222-4444\n\nIn this case, the digits and dashes 974-222-4444 are treated as just a string of characters and assigned to the variable PHONE.\n\n\n\n\n\n\nLet’s make some mistakes\n\n\n\nPaste these commands into the shell and see what happens:\n\n\nPaste these mistakes into the shell and think about what is happening.\n\nPHONE = 974-222-4444\nPHONE =974-222-4444\nPHONE= 974-222-4444\n\nWhat does this tell us about how bash interprets commands with an equals sign?\n\n\n\n\n2.1.2 Accessing variable values\n\nThe process of accessing the values stored in the variable is called “variable substitution.\nIt means: “Substitute the value for the variable where it appears on the command line.”\nIn many programming languages, you can just write a variable’s name and know that its value will be accessed, like in R:\n\nVariable <- 16\nsqrt(Variable)\n\nHowever, in bash, variable substitution is achieved by prepending $ to the variable’s name.\n\nWitness:\n\n\nPaste this into your shell\n\necho The value of PHONE is: $PHONE\n\nCool!\n\nRemember: if you make a substitution to the menu at a fancy restaurant, it is going to cost you some dollars. Same way when you make a variable substitution in bash: it costs you a dollar sign and you have to pay up front.\n\n\n\n2.1.3 Valid bash variable names\nThe bash shell demands that the names of variables:\n\nStart with _ (an underscore) or a letter\nThen include only _, letters, or numbers\n\n# good variable names\nMY_JOBS\n_Now\nSTRING\ni\ni_2\ni2\n\n# cannot be variable names\n1_node\n4THIS\nBIG-VAR\nfile-name\nSh**t!\n\n\n\n\n\n\nSelf-study\n\n\n\nChoose one of the good variable names from the list above and assign the value Good to it.\nChoose of the bad variable names from the list above and try to assign the value Bad to it.\n\n\n\n\n\n\n\n\nSelf-study answers\n\n\n\n\n\nHere is an example. {.sh filename = \"This works\"} _Now=Good\nHere is an example. {.sh filename = \"This does not work\"} BIG-VAR=Bad"
  },
  {
    "objectID": "shell-prog.html#strings-with-spaces-etc-quoting.",
    "href": "shell-prog.html#strings-with-spaces-etc-quoting.",
    "title": "2  Shell Programming",
    "section": "2.2 Strings with spaces, etc: Quoting.",
    "text": "2.2 Strings with spaces, etc: Quoting.\n\nIf you want to assign a string to a variable that has spaces in it you can quote the string, which holds it together as one “unit.”\n\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWhen the bash shell is interpreting a line of input, it breaks it into chunks called tokens which are separated by white space (spaces and TABS). If you wrap a series of words in quotation marks, it turns them all into a single token.\n\n\n\nFor example:\n\n\nPaste this into your shell\n\nMandela_Quote=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\necho $Mandela_Quote"
  },
  {
    "objectID": "shell-prog.html#variable-substitution-and-vs",
    "href": "shell-prog.html#variable-substitution-and-vs",
    "title": "2  Shell Programming",
    "section": "2.3 Variable substitution and \" vs '",
    "text": "2.3 Variable substitution and \" vs '\nWe have two types of quotes:\n\nsingle quotes, like '\ndouble quotes, like \"\n\nThey both chunk their contents into a single unit, but they behave very differently with respect to variable substition.\n\nSingle Quotes: surly and strict, you can’t substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho 'Dessert tonight is $DESSERT'\n\n\nDouble Quotes: soft and friendly, you CAN substitute values for variables inside of these.\n\nExample:\n\n\nPaste this into your shell\n\nDESSERT=\"apple pie\"\necho \"Dessert tonight is $DESSERT\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nAssign values to three shell variables, NAME, FOOD, and ACTIVITY, so that when you run the following command, it makes sense:\n\n\nAfter assigning values to the three variables, run this command\n\necho \"My name is $NAME. I like to eat $FOOD, and I enjoy $ACTIVITY.\" \n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nFor my case, I could put:\nNAME=Eric\nFOOD=\"steamed broccoli\"\nACTIVITY=\"inline skating long distances\""
  },
  {
    "objectID": "shell-prog.html#variables-are-not-just-to-be-echoed",
    "href": "shell-prog.html#variables-are-not-just-to-be-echoed",
    "title": "2  Shell Programming",
    "section": "2.4 Variables are not just to be echoed!",
    "text": "2.4 Variables are not just to be echoed!\nInvariably, when learning how to use shell variables, all the examples have you using echo to print the value of the variable. How boring and misleading.\nIt is important to understand that after a value gets substituted onto the command line, the shell goes right ahead and evaluates the resulting command line.\nSo, you can record shell variables that are command lines that do something, themselves, once they are run as a command line.\nFor example, here we make a variable whose value is the command to decompress a file to stdout:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz\"\n\nAnd now if you just substitute that variable onto the command line\n\n\nType this at the command line\n\n$MyComm\n\nthe uncompressed contents of the file data/samtools_stats/s001_stats.tsv.gz go zooming by on your screen.\n\n2.4.1 Some subtlety about evaluation of substituted values\nIf the value of the variable that is being evaluated includes pipes, redirections, or variable assignment statements, then if you just substitute it into the command line, it won’t properly be evaluated as a command line in full. For example, if MyComm was trying to decompress the file and pipe it to less, it doesn’t work as expected:\n\n\nPaste this into your shell\n\nMyComm=\"gzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\"\n\nAnd now if you just substitute that variable onto the command line the shell gets confused, because it doesn’t recognize the pipe as a pipe!\n\n\nType this at the command line\n\n$MyComm\n\nHowever, you can use the eval keyword before $MyComm to ensure that the shell recognizes that you intend for it to evaluate pipes, redirects, shell variable assignment, etc. in the substituted variable value as it normally would:\n\n\nType this at the command line\n\neval $MyComm\n\nWe will end up using this later.\nRemember, you can hit q to get out of the less page viewer."
  },
  {
    "objectID": "shell-prog.html#multiple-commands-on-one-line-with",
    "href": "shell-prog.html#multiple-commands-on-one-line-with",
    "title": "2  Shell Programming",
    "section": "2.5 Multiple commands on one line with ;",
    "text": "2.5 Multiple commands on one line with ;\n\nYou can put a ; after a command, and it will behave like a line ending—the shell will run that command, and then go to the next.\n\nExample:\necho \"Let us do this command and 2 others\"; echo \"here is number 2\"; echo \"and the third\"\nThis comes in handy.\n\n\n\n\n\n\nMore about line endings like ;\n\n\n\nThere are two other things you might find at the end of a line: & and &&\n\n& at the end of the line means “run the command, but don’t wait for it to finish.\n\nThis runs the command “in the background” in some sense.\nThis is not used very often when doing bioinformatics in a SLURM-driving system like that on Sedna\n\n&& at the end of a command means “only run the next command if the previous one did not fail.\n\nThis is very useful for making sure that you don’t keep running later commands if an earlier one failed.\nThere is also a || that is useful in this context, which is all about “exit status” of Unix commands, which is beyond our purview today.\n\n\nExamples:\n\n\nWith just semicolons...Paste it into your shell\n\necho \"Yawp before it fails.\"; ls --not-option data ; echo \"Yawp after it fails.\"\n\n\n\nWith the &&'s after each line...Paste it into your shell\n\necho \"Yawp before it fails.\" && ls --not-option data && echo \"Yawp after it fails.\""
  },
  {
    "objectID": "shell-prog.html#repetition",
    "href": "shell-prog.html#repetition",
    "title": "2  Shell Programming",
    "section": "2.6 Repetition",
    "text": "2.6 Repetition\nLet’s face it, bioinformatics, or any sort of data analysis or processing often involves doing the same thing to a number of different inputs.\nMost unix utilities are designed so that if you give it multiple inputs it will do the same thing to each and report the results in a way that is easy to understand.\nFor example, to see how many lines, words, and characters are in each Quarto (the successor to RMarkdown) document that I used to make this website, we can use wc on all the files with a .qmd extension that are one directory level above where we are currently:\n\n\nPaste this into your shell\n\nwc ../*.qmd\n\nThis is nice."
  },
  {
    "objectID": "shell-prog.html#for-loops",
    "href": "shell-prog.html#for-loops",
    "title": "2  Shell Programming",
    "section": "2.7 For loops",
    "text": "2.7 For loops\nSometimes, however, we have more complex operations to do, so we can’t just provide multiple files to a single Unix utility.\nFor example, let’s say we want to know how many lines are in each of the samtools stats files in the data/samtools_stats directory. We can’t use wc directly, because these files are gzipped, and the result we get won’t be equal to the number of lines, words, and characters in each file.\nFor repetition in these cases, bash has a for loop. Its syntax looks like this:\nfor VAR in thing1 thing2 ... thingN; do\n  one or more commands where the value of VAR is set to each of the N things in turn\ndone\nThe important “structural” parts of that are:\n\nthe for\nthe in\nthe semicolon after all the things\nthe do\nthe done\n\nHere is an example:\n\n\nPaste this into your terminal\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do \n  echo \"I like $LIKE.\"\ndone\n\nNote that this is written over multiple lines, but we can substitute ; for the ends of statements and put it all on one line. (Useful if we are just hacking away on the command line…)\n\n\nThis does the same as the above one\n\nfor LIKE in trout butterflies \"blue skies\" \"mathematical notation\"; do echo \"I like $LIKE.\"; done\n\nNote! Don’t put a semicolon after do.\n\n\n\n\n\n\nSelf study\n\n\n\nWe want to give the number of text lines, words, and characters from all the samtools_stats files.\nPrep: Here is a command that prints the name of each file.\n\n\nPaste this into the terminal\n\nfor FILE in data/samtools_stats/*.gz; do echo $FILE; done\n\nTask: I have added the -n option to the echo command which makes it not print a line ending. Your task is to replace YOUR_STUFF_HERE with an appropriate shell command to decompress each file and then print the number of lines, words, and characters in it:\n\n\nPaste this, edit YOUR_STUFF_HERE, and run it\n\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; YOUR_STUFF_HERE; done\n\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nYour edited command line should look like this:\nfor FILE in data/samtools_stats/*.gz; do echo -n $FILE; gzip -cd $FILE | wc; done"
  },
  {
    "objectID": "shell-prog.html#redirect-stdout-from-the-done",
    "href": "shell-prog.html#redirect-stdout-from-the-done",
    "title": "2  Shell Programming",
    "section": "2.8 Redirect stdout from the done",
    "text": "2.8 Redirect stdout from the done\nHere is something that is not always obvious: you can redirect or pipe the stdout of the whole for loop by using > or | immediately after the done keyword.\nUsing the example from the self study above:\n\n\nPaste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do \n  echo -n $FILE; gzip -cd $FILE | wc; \ndone > word_counts.txt\n\nNow look at what is in word_counts.txt."
  },
  {
    "objectID": "shell-prog.html#a-few-useful-topics-rapidly",
    "href": "shell-prog.html#a-few-useful-topics-rapidly",
    "title": "2  Shell Programming",
    "section": "2.9 A few useful topics, rapidly",
    "text": "2.9 A few useful topics, rapidly\n\n2.9.1 basename\n\nIf you have a path name to a file this_dir/that_dir/my_file3.txt, but you want to have the string for just the file name, my_file3.txt, you can use the basename command:\n\n\n\nTry these\n\nbasename this_dir/that_dir/my_file3.txt\nbasename ~/Documents/git-repos/CKMRpop/R/plot_conn_comps.R\n\n\n\n2.9.2 Capture stdout into a token to put on the command line\nThis is a pretty cool one, and is really nice if you want to capture a bit of output for use at a later time.\nBasically, if you run a command inside parentheses that are immediately preceded by a $, like $(command), then the stdout output of command gets put onto the command line as a single token.\nObserve:\n\n\nPaste this into your terminal\n\nSTART_TIME=$(date)\nsleep 3\nSTOP_TIME=$(date)\necho \"We started at $START_TIME, and finished at $STOP_TIME, and it is now $(date)\"\n\nOr even:\n\n\nPaste this into your terminal\n\nWCOUT=$(gzip -cd data/samtools_stats/s016_stats.tsv.gz | wc)\necho $WCOUT\n\n\n\n\n\n\n\nUnix commands we just saw:\n\n\n\n\n\n\ndate: prints the current time and date to stdout.\nsleep: causes the shell to pause for however many seconds you tell it to, like sleep 3 for three seconds, sleep 180 for three minutes."
  },
  {
    "objectID": "shell-prog.html#fancier-variable-substitution",
    "href": "shell-prog.html#fancier-variable-substitution",
    "title": "2  Shell Programming",
    "section": "2.10 Fancier variable substitution",
    "text": "2.10 Fancier variable substitution\n\n2.10.1 Wrap it in curly braces ${VAR}\nEspecially if you want to substitute a variable into a string adjacent to a letter or number or underscore, you can wrap it in curly braces.\n\n\nTry this\n\nsample=001\n# this works:\necho \"The sequences are in the file called ${sample}_seqs.fq.gz\"\n\n# this does not work the way you want it to\necho \"The sequences are in the file called $sample_seqs.fq.gz\"\n\n\n\n\n\n\n\nSelf study\n\n\n\nWhy do you think the second echo line above produced the output that it did?\n\n\n\n\n\n\n\n\nSelf Study answer\n\n\n\n\n\nSince _ is a valid character for a variable name, $sample_seqs.fq.gz gets broken up by the shell as $sample_seqs plus .fq.gz, and there is no variable named sample_seqs, so the shell just substitutes an empty string into $sample_seqs.\n\n\n\n\n\n2.10.2 Variable modification while substituting it\nbash has a whole lot of tricky syntaxes for manipulating variable values when substituting them onto the command line.\nThe one I use more than any other is ${VAR/pattern/replacement}. This looks for a text pattern pattern in the variable VAR and replaces it with the text replacement.\nHere are some examples.\n\n\nFancy substitution fun. Paste into your terminal.\n\nfile=myfile.eps\necho ${file/eps/pdf}\n\n# or maybe you want to get the sample name, s007\n# out of a file path like data/samtools_stats/s007_stats.tsv.gz\npath=data/samtools_stats/s007_stats.tsv.gz\necho $(basename ${path/_stats.tsv.gz/})\n\nWhoa! On that last one we nested a ${//} inside a $()!\n\n\n\n\n\n\nHot tip!\n\n\n\nYou can use * the way that you might when globbing filenames on the command line in the pattern for variable substitution with ${var/pattern/replacement}:\n\n\nIf we want to extract just the s001 part...\n\nSTRING=\"A-whole-lot-of-junk-before-then_s001_and-a-whole-lot_of-other-garbage.63713973\"\n\n# remove all the garbage in the beginning\nN1=${STRING/*then_/}\n\n# see what we have at this point\necho $N1\n\n# remove the remaining junk off the end\nN2=${N1/_and-*}\n\n# see what we ended up with\necho $N2\n\n\n\n\n\n2.10.3 Grouping multiple commands with (...)\n\nSometimes it is convenient lump a number of commands together into a group.\nThe main reason I do this is to capture stdout or stderr from all of them into a single file with one redirect (as opposed to redirecting (>) output from the first command, and then redirect-appending (>>) output from successive commands to the same place).\nWhen you wrap a series of commands in a pair of parentheses, they get executed as a group and you can redirect that from the right side of the last parenthesis:\n\n\n\nPaste this into your terminal\n\n(\n  echo \"This\"\n  echo \"that\"\n  echo \"and the\"\n  echo \"other\"\n) > group_it.txt\n\nCheck out the result with cat group_it.txt.\n\n\n\n\n\n\nBonus Info:\n\n\n\n\n\nWith grouping parentheses, you can also redirect stderr to a single place even if there are pipes involved. Comparing to the Self-Study answer at the end of the Quick Unix Review session:\n\n\nA stderr example\n\n(gzip -cd --bogus-option data/samtools_stats/s001_stats.tsv.gz | wc) > so.txt 2>se.txt\n\nEven though the error happened with the gzip command, the error message gets relayed through to be redirected into se.txt."
  },
  {
    "objectID": "shell-prog.html#leaving-here-for-now",
    "href": "shell-prog.html#leaving-here-for-now",
    "title": "2  Shell Programming",
    "section": "2.11 Leaving here for now…",
    "text": "2.11 Leaving here for now…\nYou might be thinking, “Wow, could I use a for loop or something like it in bash to cycle over the lines of a text file to process each line in turn?”\nThe answer, is, “You can, but bash is not always the best tool for processing text files…especially if they are large.”\nThere is a Unix utility called awk that is much better for that.\nThat is where we are heading next."
  },
  {
    "objectID": "awk-intro.html",
    "href": "awk-intro.html",
    "title": "3  A Brief awk Intro",
    "section": "",
    "text": "awk is a lighweight, completely awesome, little scripting language that is perfect for ripping through large text files with a minimal memory footprint.\nWe will only scratch the surface of it, here, but I hope it encourages awk newcomers to explore it more.\nawk can make many tasks on the command line simple, fast, and fun.\nFor a more detailed introduction, you might find this chapter of my Bioinformatics Handbook of some use. (And there are many other resources online, as well)."
  },
  {
    "objectID": "awk-intro.html#awks-philosophy-and-basic-syntax",
    "href": "awk-intro.html#awks-philosophy-and-basic-syntax",
    "title": "3  A Brief awk Intro",
    "section": "3.1 awk’s philosophy and basic syntax",
    "text": "3.1 awk’s philosophy and basic syntax\nawk is a utility that:\n\nTakes text input from a file or from stdin\nIt automatically goes line-by-line. Treating each line of text as a separate unit.\nWhen it is focused on a line of text, it breaks it into fields which you can think of as columns.\nBy default, fields, are broken up on whitespace (spaces and TABs), with multiple spaces or TABs being treated as a single delimiter.\nYou can specify the delimiter.\n\nLet’s look through a file together. I will do:\n\n\nYou can do this if you want\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | less -S\n\nand we will discuss how awk sees such a file.\n\n3.1.1 Naming fields\nOnce awk has read a line of text into memory, and split it into fields, you can access the value of those fields with special names:\n\n$1 refers to the first field (column)\n$2 refers to the second column.\n\nIf you are beyond the ninth column, the number has to be wrapped in parentheses:\n\n$(13) refers to the thirteenth column\n\n\n\n\n\n\n\nImportant\n\n\n\nThese variables within an awk script are not related to substituted variables in bash. They just happen to share a preceding $. But they are being interpreted by different programming languages (one by bash the other by awk).\n\n\n\n\n3.1.2 So what?\nSo far, this doesn’t seem very exciting, but now we learn that…\n\nYou can tell awk the sorts of lines you want it to pause at and then do some action upon it.\n\nYou tell awk which lines you want to perform an action on by matching them with a logical statement called a pattern in awk parlance.\n\nThis turns out to be incredibly powerful.\n\n\n3.1.3 awk syntax on the command line\nThe syntax for running awk with a script on the command line is like this:\n\n\nDon't try running this. It is just for explanation.\n\nawk '\n  pattern1  {action1}\n  pattern2  {action2}\n' file\n\nWhere file is the name of the file you want to process.\nOr, if you are piping text into awk from some command named cmd it would look like this:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk '\n  pattern1  {action1}\n  pattern2  {action2}\n'\n\nNote that you can pass options (like -v or -F) to awk. They go right after the awk and before the first single quotation mark.\nAlso, the carriage returns inside the single quotation marks are only there for easy reading. You could also write the last example as:\n\n\nDon't try running this. It is just for explanation.\n\ncmd | awk 'pattern1  {action1} pattern2  {action2}'\n\n…which is great if you are hacking on the command line, but once you have a lot of pattern-action pairs, it gets harder to read.\n\n\n\n\n\n\nSelf-study\n\n\n\nThinking back to the previous session when we talked about the difference between grouping strings with ' vs with \", why do you think it is important that the awk script is grouped with '?\n\n\n\n\n\n\n\n\nBrief answer\n\n\n\n\n\nAs we saw, we will be referring to different fields like $8 within the awk script. If we used \" to group the script, the shell might try to do variable substitution on any $’s in there."
  },
  {
    "objectID": "awk-intro.html#enough-talking-lets-start-doing",
    "href": "awk-intro.html#enough-talking-lets-start-doing",
    "title": "3  A Brief awk Intro",
    "section": "3.2 Enough talking, let’s start doing",
    "text": "3.2 Enough talking, let’s start doing\nAll of this will make more sense with a few examples.\n\n3.2.1 Print all the lines in which the first field is SN\nFor our first foray, let’s just pick out and print a subset of lines from our samtools stats file:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '$1==\"SN\" {print}'\n\nThat is cool.\n\n\n\n\n\n\nSelf-study\n\n\n\nMake sure that you can identify the pattern and the action in the above awk script.\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nThe pattern is $1==\"SN\"\nThe action is print\n\nThis brings up the important point that the “is equals” operator in awk is == (two consecutive equals signs…just like in R)\n\n\n\n\n\n3.2.2 Printing the lines we are interested in\nHow about if we wanted to pick out just a few particular lines from there?\nWell, we can also match lines by regular expression (which you can think of as a very fancy form of Unix word-searching.)\nLet’s say that we want information on the total number of reads mapped, the number of properly paired reads, and the total number of bases mapped using the (cigar) criterion.\nWe can see those lines in there. And we can target them by matching strings associated with them. The awk syntax puts these regular expressions in the pattern between forward slashes.\nSo, we want to match lines that have the first field equal to SN and also match other strings. We do that like this:\n\n\nPaste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print}\n  $1==\"SN\" && /reads properly paired:/ {print}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print}\n'\n\n\n\n\n\n\n\nBig Note:\n\n\n\nRegular expressions are lovely and wonderful, but occasionally frustrating. In awk’s case, parentheses have special meanings in the regular expressions, so we have to precede each one in the pattern with a backslash.\nRegular expressions are a bit beyond the scope of what we will be talking about today (entire books are devoted to the topic) but I encourage everyone to learn about them.\nThey are incredibly useful and they are used in multiple programming languages (R, python, perl, etc.)\n\n\n\n\n3.2.3 Printing just the values we are interested in\nThat is nice, but remember, we really just want to put those three values we are interested in into a table of sorts.\nSo, how do we print just the values?\nUse the fields! Count columns for each line and then print just that field:\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {print $4}\n  $1==\"SN\" && /reads properly paired:/ {print $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {print $5}\n'\n\n\n\n3.2.4 Use variables inside awk\nWe can also assign values to variables inside awk.\nThis lets us store values and then print them all on one line at the end. The special pattern END gives us a block to put actions we want to do at the very end.\n\n\nStudy this, then paste this into your terminal\n\ngzip -cd data/samtools_stats/s001_stats.tsv.gz | awk '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print rm, rpp, bmc}\n'\n\n\n\n\n\n\n\nFor the programmers out there\n\n\n\nVariables in awk are untyped and do not be declared. Basically you can put them anywhere. If code calls for the value from a variable that has not has anything assigned to it yet, the variable returns a 0 in a numerical context, and an empty string in a string context.\n\n\n\n\n3.2.5 That’s great. Can we add the sample name in there?\nYes! We can pass variables from the command line to inside awk with a -v var=value syntax.\nTo do this, we use some shell code that we learned earlier!\n\n\nStudy this, then paste this into your terminal\n\nFILE=data/samtools_stats/s001_stats.tsv.gz\ngzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n  $1==\"SN\" && /reads mapped:/ {rm = $4}\n  $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n  $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n  END {print samp, rm, rpp, bmc}\n'\n\n\n\n3.2.6 OMG! Do you seen where we are going with this?\nWe can now take that whole thing and imbed it within a bash for loop cycling over values of FILE and get the table talked about wanting in our Motivating Example when we started.\n\n\nStudy this, then paste this into your terminal\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nYowzers! That is pretty quick, and it sure beats opening each file, copying the values we want, and then pasting them into a spreadsheet."
  },
  {
    "objectID": "awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "href": "awk-intro.html#another-example-the-distribution-of-mapping-qualities",
    "title": "3  A Brief awk Intro",
    "section": "3.3 Another example: the distribution of mapping qualities",
    "text": "3.3 Another example: the distribution of mapping qualities\nHere is a fun awk example that just came up a couple of days ago for me.\n\nA colleague was telling me that she has started filtering her whole-genome sequencing BAM files so that she does not use any reads that map with a mapping quality less than 30.\nThe hope is that this will lessen batch effects.\nQuestions:\n\nWhat is the distribution of mapping quality scores in my own data\nIf we imposed such a filter, how many reads would we discard?\n\n\nIt turns out that none of the samtools programs stat, idxstats, or flagstats provide that distribution.\nThere are some other more obscure software packages that provide it, but also a lot of convoluted python code on the BioStars website for doing it.\nHa! It’s quick and easy with awk! And a great demonstration of awk’s associative arrays.\n\n3.3.1 Let’s look at an example bam file\nWe have an example bam file in the repository at data/bam/s0001.bam.\nIt only has only about 25,000 read in it so that it isn’t too large.\nLet’s have a look at it with:\n\n\nPaste this into your terminal\n\nmodule load bio/samtools\nsamtools view bam/s001.bam | less -S\n\nThe module load bio/samtools line gives us access to the samtools program, which we need for turning BAM files into text-based SAM files that we can use. Once we have given it in our shell, we have that access until we close the shell. Much more on that tomorrow!\nThe mapping quality is in the 5th field. It is a number that ranges from 0 to 60. We can count up how many times each of those numbers occurs using awk.\n\n\nHere it is all on one line as I wrote it\n\nsamtools view bam/s001.bam | awk 'BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} {n[$5]++} END {for(i in n) tot+=n[i];  for(i=60;i>=0;i--) {sum+=n[i]; print i, n[i], sum/tot;}}'\n\nAnd\n\n\nHere it is broken across lines. Paste that in your shell.\n\nsamtools view bam/s001.bam | awk '\n  BEGIN {OFS=\"\\t\"; print \"MAPQ\", \"NUM\", \"CUMUL\";} \n  {n[$5]++} \n  END {for(i in n) tot+=n[i];  for(i=60;i>=0;i--) {sum+=n[i]; print i,n[i],sum/tot;}}\n'\n\nIt’s really compact and requires very little memory to do this. (You couldn’t read a whole BAM file into R and hope to deal with it).\n\n\n\n\n\n\nAll arrays in awk are associative arrays\n\n\n\nIf you come from an R programming background, you will typically think of arrays as vectors that are indexed from 1 to n, where n is the length of the vector.\nThis is not how arrays are implemented in awk. Rather all arrays are associative arrays, which are also called hash arrays, or, in Python dictionaries. Or, if you are familiar with R, you can think of an associative array as an array that has elements that can only be accessed via their names attribute, rather than by indexing them with a number.\nSo, in awk, if we write:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[\"this\"] = \"Boing!\"\n\nThis will create an array called var (if one does not already exist) and then it will set the value of element in var that is associated with the string \"this\" to the string \"Boing!\".\nAt the same time, if you do this:\n\n\nDon't paste this anywhere! We are just talking about awk\n\nvar[30] = 67\n\nthen we are not assigning the value of 67 to the 30-th element of var. Rather, we are assigning the value 67 to the element of var that is associated with the string \"30\".\nIt can take a little getting used to, but it is very useful for counting things."
  },
  {
    "objectID": "awk-intro.html#wrap-up",
    "href": "awk-intro.html#wrap-up",
    "title": "3  A Brief awk Intro",
    "section": "3.4 Wrap-Up",
    "text": "3.4 Wrap-Up\nThat was just a brief whirlwind tour of how one can use bash and awk together to automate tasks that come up on an everyday basis when doing bioinformatics."
  },
  {
    "objectID": "awk-intro.html#looking-toward-tomorrow",
    "href": "awk-intro.html#looking-toward-tomorrow",
    "title": "3  A Brief awk Intro",
    "section": "3.5 Looking toward tomorrow",
    "text": "3.5 Looking toward tomorrow\nThe bulk of day #2 is going to be focused on working within a cluster environment, and specifically on using SLURM for launching jobs on the Sedna cluster.\nTo prepare for tomorrow, please be sure to read Chapter 8 from beginning and up to and including section 8.2. This is just a small bit to read, but it should set you up for an understanding of why and how computing clusters work differently than your desktop machine when it comes to allocating resources for computation."
  },
  {
    "objectID": "scripts-and-functions.html",
    "href": "scripts-and-functions.html",
    "title": "4  Bash scripts and functions",
    "section": "",
    "text": "Before we dive into cluster computing the Joy of SLURM, we have two last topics to cover: bash shell scripts and bash functions."
  },
  {
    "objectID": "scripts-and-functions.html#bash-shell-scripts",
    "href": "scripts-and-functions.html#bash-shell-scripts",
    "title": "4  Bash scripts and functions",
    "section": "4.1 Bash shell scripts",
    "text": "4.1 Bash shell scripts\nWe have been doing all of our bash scripting by writing commands on the command line.\nUseful bash code can be stored in a text file called a script, and then run like a normal Unix utility.\nTo illustrate this, we will copy our samtools-stats-processing commands from before into a file using the nano text editor.\nAt your command line, type this:\n\n\nType this at the command line\n\nnano sam-stats.sh\n\nThis opens a file called sam-stats.sh with a text editor called nano.\nThe convention with bash shell scripts is to give them a .sh extension, but this is not required.\nNow we copy our commands into nano.\n\n\nCopy this onto your clipboard and paste it into nano in your terminal\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nOnce that is done, use the arrow keys to go to the very top of the file and type #!/bin/bash on it and hit return twice, so that your file in nano looks like this:\n\n\nThis is what the contents of the file in nano should look like\n\n#!/bin/bash\n\nfor FILE in data/samtools_stats/*.gz; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nThen:\n\ndo cntrl-X\nAnswer Y when asked if you want to save the file\nHit return to save the file as sam-stats.sh\n\nVoila! That should exit the nano editor, and now you have a script containing that bash code.\nCheck out its contents with:\n\n\nType this at the command line\n\ncat sam-stats.sh\n\n\n\n\n\n\n\nWhat’s this #! at the top of the file?\n\n\n\nThat is colloquially referred to as the shebang line.\nA # usually tells the shell to “ignore everything to the right of the # on this line”\n# is used to precede comments in your code.\nHowever, in this case, at the top of the file and followed by a ! it tells the computer what language to use to interpret this file.\nIn our case /bin/bash is where the bash shell interpreter typically is found on most Unix or Linux system.\n(If bash is the default shell on your system, you may not always need to have the shebang line, but it is good practice to do so.)\n\n\n\n4.1.1 Script files must be executable\nThe Unix operating system distinguishes between files that just hold data, and files that can the run or be “executed” by the computer.\nFor your bash commands in a script to run on the computer, it must be of an executable type.\nWe can make the file executable using the chmod command, like this:\n\n\nPaste this into your shell\n\nchmod u+x sam-stats.sh\n\nIf we then use ls -l to list the file in long format like this:\n\n\nType this in\n\nls -l sam-stats.sh\n\nwe see:\n-rwxrw-r-- 1 eanderson eanderson 317 Oct 14 13:40 sam-stats.sh\nThe x in the first field of that line indicates that the file is executable by the user.\n\n\n4.1.2 Running a script\nTo run a script that is executable, you type the path to it.\nOn Sedna, by default, the current working directory is not a place the computer looks for executable scripts, so we have to prepend ./ to its path:\n\n\nType this on the command line and hit RETURN.\n\n./sam-stats.sh\n\nThat runs our script.\n\n\n4.1.3 Scripts are more useful if you can specify the inputs\nSo, that runs our script and produces results, but that is not so useful. We already had those results, in a sense.\nShell scripts become much more useful when you can change the inputs that go to them.\nOne way to do so involves using positional parameters\n\n\n4.1.4 Arguments following a script can be accessed within the script\nIf you put arguments after a script on the command line, then in the script itself:\n\nthe value of the first argument is accessible as $1\nthe value of the second argument is accessible as $2\n\n…and so forth for as many arguments as you want.\nSo, we can rewrite our script as:\n\n\nJust give this a read\n\n#!/bin/bash\n\nfor FILE in $1; do\n  gzip -cd $FILE | awk -v samp=$(basename ${FILE/_stats.tsv.gz/}) '\n    $1==\"SN\" && /reads mapped:/ {rm = $4}\n    $1==\"SN\" && /reads properly paired:/ {rpp = $5}\n    $1==\"SN\" && /bases mapped \\(cigar\\):/ {bmc = $5}\n    END {print samp, rm, rpp, bmc}\n  '\ndone\n\nSee that we have replaced data/samtools_stats/*.gz with $1.\nWe actually have that very script stored in scripts/sam-stat-pp.sh. And you can use it like this:\n\n\nPaste this into the shell\n\n./scripts/sam-stat-pp.sh \"data/samtools_stats/s0*.gz\"\n\nHere, we have passed in \"data/samtools_stats/s0*.gz\" as the first argument to the script, and, since we have a $1 in the script where that goes, it does what it did before.\n\n\n4.1.5 Now we can use it on other samtools stats files\nIt is not very exciting to see it just run again on the same set of files.\nBut, now we could direct the script to operate on a different set of files, just by changing the argument that we pass to the script.\nI have put a much larger set of samtools stats files within subdirectory of the /share directory on Sedna. You should be able to list them all with:\n\n\nPaste this into your shell. This should work...\n\n ls /share/all/eriq/big_sam_stats/s*.gz\n\nWhoa! 275 files. (They are here on Sedna, in the shared folder, because I didn’t want to put them all on GitHub.)\nBut, now, we can summarize them all just by pointing our script to them:\n\n\nThis should work on Sedna, Paste it into your shell\n\n./scripts/sam-stat-pp.sh \"/share/all/eriq/big_sam_stats/s*.gz\"\n\nThat is fast.\nIf we wanted to redirect that into a file we could do so easily."
  },
  {
    "objectID": "scripts-and-functions.html#bash-functions",
    "href": "scripts-and-functions.html#bash-functions",
    "title": "4  Bash scripts and functions",
    "section": "4.2 Bash functions",
    "text": "4.2 Bash functions\nLike most programming languages, you can define functions in bash.\nThe syntax for defining a function named MyFunc is:\n\n\nJust read this\n\nfunction MyFunc {\n  ...code that MyFunc executes...\n}\n\nIn this regard, it has a similar syntax to R.\nOnce defined, you can use the name of the function (in the above case, MyFunc) as if it were just another Unix command.\n\n4.2.1 Example: congratulate yourself on making it through your day\nHere is a gratuitous example: we make a bash function called congrats that tells us what time it is and encourages us to keep getting through our day:\n\n\nPaste this into your terminal\n\nfunction congrats { echo \"It is now $(date).  Congrats on making it this far in your day.\"; }\n\n\n\n\n\n\n\nWarning\n\n\n\nCurly braces in bash are extremely finicky. They don’t like to be near other characters. In the above, the space after the { is critical, as is the ; before the }. (The last } needs to have a ; before it if it does not have a line ending before it).\n\n\n\n\n4.2.2 Bash functions take positional parameters too\nWe can rewrite our function as congrats2 so that it can use two arguments, the first a name, and the second an adjective:\n\n\nPaste this into your terminal\n\nfunction congrats2 { \n  echo \"It is now $(date).  Congrats, $1, you are $2.\"\n}\n\nNow, you can use that function and supply it with whatever names and adjectives you would like:\n\n\nPaste these in and see what it does.\n\ncongrats2 Fred splendid\ncongrats2 Eric tired\ncongrats2 Amy amazing\n\nWe will write a few functions, later, to simplify our life in SLURM."
  },
  {
    "objectID": "slurm.html",
    "href": "slurm.html",
    "title": "5  SLURM",
    "section": "",
    "text": "SLURM stands for “Simple Linux Utility for Resource Management”\nIt is the “queueing system” used to equitable provide compute resources on the Sedna HPCC."
  },
  {
    "objectID": "slurm.html#why-do-we-need-slurm",
    "href": "slurm.html#why-do-we-need-slurm",
    "title": "5  SLURM",
    "section": "5.1 Why do we need SLURM?",
    "text": "5.1 Why do we need SLURM?\n\nThe fundamental problem of cluster computing.\nA cluster does not operate like your laptop.\nMost compute-intensive jobs run most efficiently on a dedicated processor or processors."
  },
  {
    "objectID": "slurm.html#hpcc-architecture-in-a-nutshell",
    "href": "slurm.html#hpcc-architecture-in-a-nutshell",
    "title": "5  SLURM",
    "section": "5.2 HPCC Architecture in a Nutshell",
    "text": "5.2 HPCC Architecture in a Nutshell\n\n\nNodes: the closest thing to what you think of as a computer. (“Pizza-boxes” with no displays).\n\nEach node is attached via a fast connection to centralized attached storage (A big set of hard drives attached via “Infiniband.”)\nWithin each node are some numbers of Cores or CPUs\n\nCores/CPUs are the actual processing units within a node. (Usually 20 to 24)\n\n\nSedna, like almost all other HPCCs has a login node\n\nThe login node is dedicated to allowing people to communicate with HPCC.\nDO NOT do computationally intensive, or input/output-intensive jobs on the login node\nNot surprisingly, when you login to Sedna you are on the login node.\n\n\n\n\n\n\n\n\nHot tip!\n\n\n\nIn the default configuration on Sedna, your command prompt at the shell tells you which node you are logged into:"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "eca-setup-notes.html",
    "href": "eca-setup-notes.html",
    "title": "7  Notes on Eric’s Setup Process",
    "section": "",
    "text": "These are just notes to myself. People shouldn’t follow these steps…\nI copied my existing ones into ~/BACKUP_PROFILES, then put the default ones into place:\n(base) [sedna: ~]--% cp /etc/skel/.bashrc ./\n(base) [sedna: ~]--% cp /etc/skel/.bash_profile ./\nThen I logged out and back in. Then I ran this line:\neval \"$(/opt/bioinformatics/miniconda3/bin/conda shell.bash hook)\"\nAnd I noted that in this base environment, mamba is available.\nI also listed all the environments, and I have a boatload in my home directory, but the main one that everyone is going to need is:\n/opt/bioinformatics/miniconda3/envs/snakemake-7.7.0\nSo, after that, I added the eval \"$(/opt/bioinformatics/miniconda3/bin/conda shell.bash hook)\" line to my .bashrc and then logged out and back in.\nThen, I tried this:L\n# in: /home/eanderson/Documents/git-repos/nmfs-bioinf-2022/Snakemake-Example\nconda activate /opt/bioinformatics/miniconda3/envs/snakemake-7.7.0\nrm -rf results resources/genome.fasta.* resources/genome.dict\n\n# then\nsnakemake -np --cores 20 --use-envmodules results/vcf/all.vcf\n\n# that knew just what to do.  And I tried that on a node with 20 cores\nsnakemake  --cores 20 --use-envmodules results/vcf/all.vcf"
  }
]